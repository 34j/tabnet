{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "import os\n",
    "import wget\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download census-income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "dataset_name = 'census-income'\n",
    "out = Path(os.getcwd()+'/data/'+dataset_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "out.parent.mkdir(parents=True, exist_ok=True)\n",
    "if out.exists():\n",
    "    print(\"File already exists.\")\n",
    "else:\n",
    "    print(\"Downloading file...\")\n",
    "    wget.download(url, out.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 'train_bench_gmsc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(out)\n",
    "target = 'SeriousDlqin2yrs'  # ' <=50K'\n",
    "if \"Set\" not in df.columns:\n",
    "    df[\"Set\"] = np.random.choice([\"train\", \"test\"], p =[.8, .2], size=(df.shape[0],))\n",
    "\n",
    "train_indices = df[df.Set==\"train\"].index\n",
    "test_indices = df[df.Set==\"test\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unamed', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[df.Set==\"train\"].reset_index(drop=True)\n",
    "test = df.loc[df.Set==\"test\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "      <th>Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.550178</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.383287</td>\n",
       "      <td>10829.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.097876</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>77000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.028904</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.012218</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.134993</td>\n",
       "      <td>6666.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.191007</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0.619689</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  age  \\\n",
       "0                 0                              0.550178   58   \n",
       "1                 0                              0.097876   82   \n",
       "2                 0                              0.028904   62   \n",
       "3                 1                              0.012218   53   \n",
       "4                 0                              0.191007   48   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                     1   0.383287        10829.0   \n",
       "1                                     0   0.006454        77000.0   \n",
       "2                                     0   0.004142         7000.0   \n",
       "3                                     1   0.134993         6666.0   \n",
       "4                                     3   0.619689         5200.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                               12                        0   \n",
       "1                                7                        0   \n",
       "2                                7                        0   \n",
       "3                                6                        0   \n",
       "4                                8                        0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                             2                                     0   \n",
       "1                             1                                     0   \n",
       "2                             0                                     0   \n",
       "3                             0                                     0   \n",
       "4                             1                                     0   \n",
       "\n",
       "   NumberOfDependents    Set  \n",
       "0                 2.0  train  \n",
       "1                 0.0  train  \n",
       "2                 0.0  train  \n",
       "3                 1.0  train  \n",
       "4                 1.0  train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple preprocessing\n",
    "\n",
    "Label encode categorical features and fill empty cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, nan_value='VV_likely', other_values='VV_UOthER'):\n",
    "        super().__init__()\n",
    "        self.nan_value = nan_value\n",
    "        self.other_values = other_values\n",
    "        self.l_enc = LabelEncoder()\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.l_enc.fit(pd.concat([X.astype(str), pd.Series([self.nan_value, self.other_values])]))\n",
    "        self.classes_ = self.l_enc.classes_\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.astype(str).apply(lambda x: self.other_values if (x not in self.l_enc.classes_ and not pd.isnull(x)) else x)\n",
    "        X = X.fillna(self.nan_value)\n",
    "        return self.l_enc.transform(X)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1\n"
     ]
    }
   ],
   "source": [
    "nunique = train.nunique()\n",
    "types = train.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in train.columns:\n",
    "    if types[col] == 'object':\n",
    "        print(col, train[col].nunique())\n",
    "        l_enc = Encoder()\n",
    "        train[col] = l_enc.fit_transform(train[col])\n",
    "        test[col] = l_enc.transform(test[col])\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)\n",
    "    else:\n",
    "        qt = QuantileTransformer()\n",
    "        train[col].fillna(train[col].min() - 1, inplace=True)\n",
    "        test[col].fillna(train[col].min() - 1, inplace=True)\n",
    "        \n",
    "        train[col] = qt.fit_transform(train[col].values.reshape(-1, 1))\n",
    "        test[col] = qt.transform(test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define categorical features for categorical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_feat = ['Set']\n",
    "\n",
    "features = [ col for col in train.columns if col not in unused_feat+[target]] \n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[features].values\n",
    "y_train = train[target].values\n",
    "\n",
    "X_test = test[features].values\n",
    "y_test = test[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_preprocess(X_train, y_train, n_splits=8, random_state=0):\n",
    "   \n",
    "    \n",
    "    # CVSplit = KFold if self.learning_task == 'regression' else StratifiedKFold\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    for train_index, test_index in cv.split(X_train, y_train):\n",
    "        train, test = X_train[train_index], X_train[test_index]\n",
    "        train, ytr = train.astype(float), y_train[train_index]\n",
    "        test, yte = test.astype(float), y_train[test_index]\n",
    "        yield train.astype(float), ytr, test.astype(float), yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 20 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.23144 |  0.21069 |   1.9       \n",
      "| 2     | 0.19231 |  0.19909 |   3.8       \n",
      "| 3     | 0.18825 |  0.18774 |   5.9       \n",
      "| 4     | 0.18571 |  0.18601 |   7.8       \n",
      "| 5     | 0.18505 |  0.18426 |   9.7       \n",
      "| 6     | 0.18445 |  0.18184 |   11.5      \n",
      "| 7     | 0.18306 |  0.17974 |   13.2      \n",
      "| 8     | 0.18262 |  0.18086 |   15.0      \n",
      "| 9     | 0.18248 |  0.18026 |   16.7      \n",
      "| 10    | 0.18250 |  0.17969 |   18.5      \n",
      "| 11    | 0.18212 |  0.18118 |   20.2      \n",
      "| 12    | 0.18285 |  0.18147 |   21.9      \n",
      "| 13    | 0.18289 |  0.18152 |   23.7      \n",
      "| 14    | 0.18265 |  0.18119 |   25.5      \n",
      "| 15    | 0.18258 |  0.18024 |   27.3      \n",
      "| 16    | 0.18132 |  0.17993 |   29.0      \n",
      "| 17    | 0.18072 |  0.17944 |   30.8      \n",
      "| 18    | 0.18109 |  0.18021 |   32.6      \n",
      "| 19    | 0.18078 |  0.18027 |   34.4      \n",
      "| 20    | 0.18033 |  0.17956 |   36.1      \n",
      "| 21    | 0.18103 |  0.17982 |   37.9      \n",
      "| 22    | 0.18102 |  0.17915 |   39.7      \n",
      "| 23    | 0.18031 |  0.17972 |   41.4      \n",
      "| 24    | 0.18015 |  0.17992 |   43.2      \n",
      "| 25    | 0.18133 |  0.18151 |   45.0      \n",
      "| 26    | 0.18117 |  0.17990 |   46.8      \n",
      "| 27    | 0.18021 |  0.17950 |   48.6      \n",
      "| 28    | 0.18008 |  0.17910 |   50.4      \n",
      "| 29    | 0.18092 |  0.17899 |   52.1      \n",
      "| 30    | 0.18018 |  0.18006 |   53.9      \n",
      "| 31    | 0.18005 |  0.17994 |   55.7      \n",
      "| 32    | 0.18012 |  0.18034 |   57.4      \n",
      "| 33    | 0.18011 |  0.17926 |   59.2      \n",
      "| 34    | 0.18019 |  0.18008 |   60.9      \n",
      "| 35    | 0.18010 |  0.18041 |   62.7      \n",
      "| 36    | 0.18025 |  0.17984 |   64.4      \n",
      "| 37    | 0.17986 |  0.17962 |   66.2      \n",
      "| 38    | 0.17977 |  0.17890 |   68.0      \n",
      "| 39    | 0.17971 |  0.17918 |   69.8      \n",
      "| 40    | 0.17944 |  0.17976 |   71.6      \n",
      "| 41    | 0.17988 |  0.17962 |   73.4      \n",
      "| 42    | 0.18024 |  0.18073 |   75.2      \n",
      "| 43    | 0.18091 |  0.18039 |   77.0      \n",
      "| 44    | 0.17978 |  0.17900 |   78.7      \n",
      "| 45    | 0.17989 |  0.17893 |   80.5      \n",
      "| 46    | 0.18018 |  0.18008 |   82.3      \n",
      "| 47    | 0.18032 |  0.18016 |   84.1      \n",
      "| 48    | 0.18019 |  0.18053 |   85.8      \n",
      "| 49    | 0.17999 |  0.18017 |   87.6      \n",
      "| 50    | 0.17984 |  0.17965 |   89.4      \n",
      "| 51    | 0.17948 |  0.18051 |   91.1      \n",
      "| 52    | 0.18014 |  0.17974 |   92.9      \n",
      "| 53    | 0.18038 |  0.18184 |   94.7      \n",
      "| 54    | 0.18055 |  0.17985 |   96.5      \n",
      "| 55    | 0.17984 |  0.18155 |   98.2      \n",
      "| 56    | 0.17971 |  0.17955 |   100.0     \n",
      "| 57    | 0.17938 |  0.18128 |   101.8     \n",
      "| 58    | 0.18016 |  0.18001 |   103.6     \n",
      "Early stopping occured at epoch 58\n",
      "Training done in 103.560 seconds.\n",
      "---------------------------------------\n",
      "0.178898919064148\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 20 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.22746 |  0.20323 |   1.7       \n",
      "| 2     | 0.18915 |  0.19673 |   3.3       \n",
      "| 3     | 0.18642 |  0.19293 |   5.0       \n",
      "| 4     | 0.18597 |  0.18675 |   6.6       \n",
      "| 5     | 0.18481 |  0.18525 |   8.3       \n",
      "| 6     | 0.18522 |  0.18408 |   10.0      \n",
      "| 7     | 0.18376 |  0.18371 |   11.7      \n",
      "| 8     | 0.18318 |  0.18179 |   13.4      \n",
      "| 9     | 0.18295 |  0.18302 |   15.1      \n",
      "| 10    | 0.18303 |  0.18163 |   16.8      \n",
      "| 11    | 0.18235 |  0.18153 |   18.4      \n",
      "| 12    | 0.18270 |  0.18174 |   20.1      \n",
      "| 13    | 0.18236 |  0.18246 |   21.7      \n",
      "| 14    | 0.18314 |  0.18105 |   23.4      \n",
      "| 15    | 0.18227 |  0.18334 |   25.1      \n",
      "| 16    | 0.18286 |  0.18094 |   26.8      \n",
      "| 17    | 0.18217 |  0.18191 |   28.4      \n",
      "| 18    | 0.18235 |  0.18178 |   30.0      \n",
      "| 19    | 0.18161 |  0.18029 |   31.7      \n",
      "| 20    | 0.18098 |  0.18200 |   33.3      \n",
      "| 21    | 0.18138 |  0.18033 |   34.9      \n",
      "| 22    | 0.18217 |  0.18215 |   36.6      \n",
      "| 23    | 0.18207 |  0.18086 |   38.2      \n",
      "| 24    | 0.18127 |  0.18099 |   39.9      \n",
      "| 25    | 0.18138 |  0.18087 |   41.6      \n",
      "| 26    | 0.18136 |  0.18198 |   43.2      \n",
      "| 27    | 0.18149 |  0.18072 |   44.9      \n",
      "| 28    | 0.18208 |  0.18191 |   46.5      \n",
      "| 29    | 0.18184 |  0.18243 |   48.2      \n",
      "| 30    | 0.18153 |  0.18046 |   49.8      \n",
      "| 31    | 0.18151 |  0.18091 |   51.4      \n",
      "| 32    | 0.18150 |  0.17989 |   53.1      \n",
      "| 33    | 0.18113 |  0.18141 |   54.7      \n",
      "| 34    | 0.18166 |  0.18036 |   56.4      \n",
      "| 35    | 0.18166 |  0.18151 |   58.1      \n",
      "| 36    | 0.18154 |  0.18131 |   59.7      \n",
      "| 37    | 0.18239 |  0.18118 |   61.4      \n",
      "| 38    | 0.18141 |  0.18076 |   63.0      \n",
      "| 39    | 0.18155 |  0.18062 |   64.7      \n",
      "| 40    | 0.18140 |  0.18180 |   66.4      \n",
      "| 41    | 0.18172 |  0.18213 |   68.0      \n",
      "| 42    | 0.18102 |  0.18076 |   69.6      \n",
      "| 43    | 0.18210 |  0.18022 |   71.3      \n",
      "| 44    | 0.18159 |  0.18055 |   72.9      \n",
      "| 45    | 0.18176 |  0.18123 |   74.6      \n",
      "| 46    | 0.18164 |  0.18205 |   76.7      \n",
      "| 47    | 0.18126 |  0.18049 |   78.8      \n",
      "| 48    | 0.18111 |  0.18063 |   80.5      \n",
      "| 49    | 0.18069 |  0.18061 |   82.3      \n",
      "| 50    | 0.18121 |  0.17999 |   84.2      \n",
      "| 51    | 0.18142 |  0.18027 |   86.0      \n",
      "| 52    | 0.18144 |  0.18111 |   87.7      \n",
      "Early stopping occured at epoch 52\n",
      "Training done in 87.696 seconds.\n",
      "---------------------------------------\n",
      "0.1798895315900057\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 20 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.23370 |  0.21175 |   2.0       \n",
      "| 2     | 0.18893 |  0.20617 |   3.8       \n",
      "| 3     | 0.18596 |  0.19647 |   5.6       \n",
      "| 4     | 0.18519 |  0.19007 |   7.4       \n",
      "| 5     | 0.18401 |  0.18844 |   9.2       \n",
      "| 6     | 0.18354 |  0.18974 |   10.8      \n",
      "| 7     | 0.18288 |  0.18921 |   12.4      \n",
      "| 8     | 0.18310 |  0.18754 |   14.1      \n",
      "| 9     | 0.18398 |  0.18801 |   15.8      \n",
      "| 10    | 0.18324 |  0.18784 |   17.5      \n",
      "| 11    | 0.18232 |  0.18902 |   19.1      \n",
      "| 12    | 0.18155 |  0.18682 |   20.8      \n",
      "| 13    | 0.18079 |  0.18720 |   22.5      \n",
      "| 14    | 0.18046 |  0.18539 |   24.1      \n",
      "| 15    | 0.18070 |  0.18602 |   25.8      \n",
      "| 16    | 0.18069 |  0.18505 |   27.5      \n",
      "| 17    | 0.18102 |  0.18568 |   29.3      \n",
      "| 18    | 0.18060 |  0.18633 |   31.0      \n",
      "| 19    | 0.18006 |  0.18637 |   32.7      \n",
      "| 20    | 0.18028 |  0.18463 |   34.4      \n",
      "| 21    | 0.17961 |  0.18632 |   36.0      \n",
      "| 22    | 0.17980 |  0.18599 |   37.7      \n",
      "| 23    | 0.18053 |  0.18422 |   39.4      \n",
      "| 24    | 0.17943 |  0.18317 |   41.1      \n",
      "| 25    | 0.17979 |  0.18504 |   42.7      \n",
      "| 26    | 0.17939 |  0.18341 |   44.4      \n",
      "| 27    | 0.17912 |  0.18358 |   46.1      \n",
      "| 28    | 0.17955 |  0.18342 |   47.7      \n",
      "| 29    | 0.17924 |  0.18347 |   49.4      \n",
      "| 30    | 0.17952 |  0.18387 |   51.0      \n",
      "| 31    | 0.17920 |  0.18304 |   52.8      \n",
      "| 32    | 0.17919 |  0.18268 |   54.4      \n",
      "| 33    | 0.17927 |  0.18389 |   56.1      \n",
      "| 34    | 0.17900 |  0.18342 |   57.8      \n",
      "| 35    | 0.17929 |  0.18359 |   59.4      \n",
      "| 36    | 0.17926 |  0.18276 |   61.1      \n",
      "| 37    | 0.17913 |  0.18333 |   62.8      \n",
      "| 38    | 0.17897 |  0.18314 |   64.4      \n",
      "| 39    | 0.17884 |  0.18411 |   66.1      \n",
      "| 40    | 0.17939 |  0.18494 |   67.7      \n",
      "| 41    | 0.17899 |  0.18424 |   69.4      \n",
      "| 42    | 0.17915 |  0.18370 |   71.0      \n",
      "| 43    | 0.17909 |  0.18361 |   72.7      \n",
      "| 44    | 0.17878 |  0.18371 |   74.3      \n",
      "| 45    | 0.17879 |  0.18365 |   76.0      \n",
      "| 46    | 0.17850 |  0.18398 |   77.7      \n",
      "| 47    | 0.17867 |  0.18346 |   79.3      \n",
      "| 48    | 0.17835 |  0.18318 |   81.0      \n",
      "| 49    | 0.17831 |  0.18292 |   82.6      \n",
      "| 50    | 0.17842 |  0.18345 |   84.2      \n",
      "| 51    | 0.17846 |  0.18617 |   85.9      \n",
      "| 52    | 0.17870 |  0.18293 |   87.5      \n",
      "Early stopping occured at epoch 52\n",
      "Training done in 87.522 seconds.\n",
      "---------------------------------------\n",
      "0.18267767277283906\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 20 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 1     | 0.22957 |  0.21300 |   1.7       \n",
      "| 2     | 0.19124 |  0.20799 |   3.3       \n",
      "| 3     | 0.18657 |  0.20421 |   5.0       \n",
      "| 4     | 0.18554 |  0.19641 |   6.6       \n",
      "| 5     | 0.18427 |  0.18722 |   8.4       \n",
      "| 6     | 0.18365 |  0.18826 |   10.0      \n",
      "| 7     | 0.18318 |  0.18188 |   11.7      \n",
      "| 8     | 0.18275 |  0.18171 |   13.4      \n",
      "| 9     | 0.18185 |  0.18109 |   15.1      \n",
      "| 10    | 0.18177 |  0.18178 |   16.8      \n",
      "| 11    | 0.18149 |  0.18142 |   18.5      \n",
      "| 12    | 0.18109 |  0.18104 |   20.3      \n",
      "| 13    | 0.18105 |  0.18035 |   22.0      \n",
      "| 14    | 0.18063 |  0.18208 |   23.8      \n",
      "| 15    | 0.18107 |  0.18031 |   25.6      \n",
      "| 16    | 0.18102 |  0.18156 |   27.5      \n",
      "| 17    | 0.18076 |  0.18014 |   29.4      \n",
      "| 18    | 0.18072 |  0.18140 |   31.2      \n",
      "| 19    | 0.18081 |  0.18008 |   33.0      \n",
      "| 20    | 0.18044 |  0.17965 |   34.7      \n",
      "| 21    | 0.18062 |  0.18015 |   36.5      \n",
      "| 22    | 0.18066 |  0.18245 |   38.4      \n",
      "| 23    | 0.18056 |  0.18037 |   40.3      \n",
      "| 24    | 0.18080 |  0.17945 |   42.1      \n",
      "| 25    | 0.18027 |  0.17942 |   43.9      \n",
      "| 26    | 0.17970 |  0.17980 |   45.7      \n",
      "| 27    | 0.18008 |  0.18125 |   47.5      \n",
      "| 28    | 0.18141 |  0.17958 |   49.4      \n",
      "| 29    | 0.18090 |  0.18019 |   51.3      \n",
      "| 30    | 0.18038 |  0.17970 |   53.0      \n",
      "| 31    | 0.18067 |  0.18037 |   54.9      \n",
      "| 32    | 0.18027 |  0.17948 |   57.0      \n",
      "| 33    | 0.18054 |  0.17946 |   59.0      \n",
      "| 34    | 0.18028 |  0.17852 |   61.1      \n",
      "| 35    | 0.18008 |  0.18064 |   63.1      \n",
      "| 36    | 0.18043 |  0.17854 |   64.9      \n",
      "| 37    | 0.17990 |  0.17907 |   66.6      \n",
      "| 38    | 0.18049 |  0.17975 |   68.4      \n",
      "| 39    | 0.18005 |  0.18006 |   70.2      \n",
      "| 40    | 0.17987 |  0.17923 |   71.9      \n",
      "| 41    | 0.17957 |  0.17975 |   73.7      \n",
      "| 42    | 0.17992 |  0.17995 |   75.3      \n",
      "| 43    | 0.18027 |  0.18102 |   77.0      \n",
      "| 44    | 0.17979 |  0.17957 |   78.7      \n",
      "| 45    | 0.18050 |  0.17978 |   80.4      \n",
      "| 46    | 0.17995 |  0.18126 |   82.1      \n",
      "| 47    | 0.18002 |  0.18244 |   83.7      \n",
      "| 48    | 0.18082 |  0.18286 |   85.4      \n",
      "| 49    | 0.18060 |  0.18004 |   87.1      \n",
      "| 50    | 0.18042 |  0.17951 |   88.8      \n",
      "| 51    | 0.17980 |  0.17753 |   90.4      \n",
      "| 52    | 0.17956 |  0.17868 |   92.1      \n",
      "| 53    | 0.17956 |  0.17919 |   93.8      \n",
      "| 54    | 0.17913 |  0.17818 |   95.4      \n",
      "| 55    | 0.17924 |  0.17941 |   97.1      \n",
      "| 56    | 0.17935 |  0.17766 |   98.8      \n",
      "| 57    | 0.17943 |  0.17828 |   100.5     \n",
      "| 58    | 0.17913 |  0.17816 |   102.2     \n",
      "| 59    | 0.17912 |  0.17903 |   103.9     \n",
      "| 60    | 0.17905 |  0.17834 |   105.6     \n",
      "| 61    | 0.17874 |  0.17801 |   107.3     \n",
      "| 62    | 0.17927 |  0.17835 |   109.0     \n",
      "| 63    | 0.17888 |  0.17862 |   110.7     \n",
      "| 64    | 0.17897 |  0.17842 |   112.3     \n",
      "| 65    | 0.18007 |  0.17997 |   114.0     \n",
      "| 66    | 0.17930 |  0.17985 |   115.6     \n",
      "| 67    | 0.18082 |  0.17975 |   117.3     \n",
      "| 68    | 0.17880 |  0.17975 |   118.9     \n",
      "| 69    | 0.17879 |  0.17856 |   120.7     \n",
      "| 70    | 0.17935 |  0.17842 |   122.3     \n",
      "| 71    | 0.17898 |  0.17851 |   124.0     \n",
      "Early stopping occured at epoch 71\n",
      "Training done in 124.029 seconds.\n",
      "---------------------------------------\n",
      "0.17753192847183286\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 20 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.22945 |  0.21696 |   1.7       \n",
      "| 2     | 0.19091 |  0.20335 |   3.4       \n",
      "| 3     | 0.18695 |  0.19724 |   5.1       \n",
      "| 4     | 0.18528 |  0.18997 |   6.8       \n",
      "| 5     | 0.18343 |  0.18707 |   8.5       \n",
      "| 6     | 0.18250 |  0.18463 |   10.2      \n",
      "| 7     | 0.18270 |  0.18325 |   11.8      \n",
      "| 8     | 0.18133 |  0.18154 |   13.5      \n",
      "| 9     | 0.18172 |  0.18264 |   15.2      \n",
      "| 10    | 0.18119 |  0.18252 |   16.9      \n",
      "| 11    | 0.18091 |  0.18182 |   18.6      \n",
      "| 12    | 0.18159 |  0.18176 |   20.3      \n",
      "| 13    | 0.18163 |  0.18127 |   22.0      \n",
      "| 14    | 0.18091 |  0.18105 |   23.7      \n",
      "| 15    | 0.18088 |  0.18053 |   25.5      \n",
      "| 16    | 0.18064 |  0.18054 |   27.1      \n",
      "| 17    | 0.18048 |  0.18044 |   28.8      \n",
      "| 18    | 0.18051 |  0.18068 |   30.5      \n",
      "| 19    | 0.18040 |  0.18057 |   32.1      \n",
      "| 20    | 0.18033 |  0.18122 |   33.8      \n",
      "| 21    | 0.18059 |  0.18145 |   35.5      \n",
      "| 22    | 0.18237 |  0.18136 |   37.1      \n",
      "| 23    | 0.18086 |  0.18211 |   38.8      \n",
      "| 24    | 0.18047 |  0.18131 |   40.5      \n",
      "| 25    | 0.18015 |  0.18026 |   42.2      \n",
      "| 26    | 0.18009 |  0.18044 |   43.8      \n",
      "| 27    | 0.18048 |  0.18096 |   45.5      \n",
      "| 28    | 0.18031 |  0.18063 |   47.2      \n",
      "| 29    | 0.18037 |  0.18024 |   48.9      \n",
      "| 30    | 0.17990 |  0.18077 |   50.6      \n",
      "| 31    | 0.18034 |  0.18100 |   52.3      \n",
      "| 32    | 0.18017 |  0.17966 |   54.0      \n",
      "| 33    | 0.17963 |  0.18089 |   55.6      \n",
      "| 34    | 0.17991 |  0.18085 |   57.3      \n",
      "| 35    | 0.18011 |  0.18043 |   59.0      \n",
      "| 36    | 0.18011 |  0.18102 |   60.6      \n",
      "| 37    | 0.17957 |  0.18131 |   62.3      \n",
      "| 38    | 0.17997 |  0.17929 |   64.0      \n",
      "| 39    | 0.17966 |  0.18045 |   65.7      \n",
      "| 40    | 0.17972 |  0.18078 |   67.3      \n",
      "| 41    | 0.18002 |  0.17969 |   69.0      \n",
      "| 42    | 0.18036 |  0.18048 |   70.7      \n",
      "| 43    | 0.17973 |  0.18026 |   72.4      \n",
      "| 44    | 0.17991 |  0.17971 |   74.1      \n",
      "| 45    | 0.18020 |  0.17932 |   75.8      \n",
      "| 46    | 0.17936 |  0.18121 |   77.5      \n",
      "| 47    | 0.17934 |  0.18027 |   79.2      \n",
      "| 48    | 0.17981 |  0.17994 |   81.2      \n",
      "| 49    | 0.17906 |  0.17947 |   83.2      \n",
      "| 50    | 0.17985 |  0.18125 |   84.8      \n",
      "| 51    | 0.17963 |  0.18078 |   86.5      \n",
      "| 52    | 0.17935 |  0.17985 |   88.1      \n",
      "| 53    | 0.17909 |  0.18034 |   89.8      \n",
      "| 54    | 0.17890 |  0.18021 |   91.5      \n",
      "| 55    | 0.17931 |  0.18196 |   93.2      \n",
      "| 56    | 0.17924 |  0.17931 |   94.8      \n",
      "| 57    | 0.17866 |  0.17971 |   96.5      \n",
      "| 58    | 0.17877 |  0.18000 |   98.1      \n",
      "Early stopping occured at epoch 58\n",
      "Training done in 98.133 seconds.\n",
      "---------------------------------------\n",
      "0.1792949314774635\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 20 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.23340 |  0.20492 |   1.7       \n",
      "| 2     | 0.19469 |  0.19628 |   3.3       \n",
      "| 3     | 0.19168 |  0.19132 |   5.0       \n",
      "| 4     | 0.18853 |  0.18748 |   6.6       \n",
      "| 5     | 0.18621 |  0.18356 |   8.3       \n",
      "| 6     | 0.18416 |  0.18164 |   9.9       \n",
      "| 7     | 0.18360 |  0.18106 |   11.6      \n",
      "| 8     | 0.18386 |  0.17961 |   13.3      \n",
      "| 9     | 0.18322 |  0.17931 |   14.9      \n",
      "| 10    | 0.18289 |  0.17889 |   16.6      \n",
      "| 11    | 0.18339 |  0.17933 |   18.3      \n",
      "| 12    | 0.18283 |  0.17980 |   20.0      \n",
      "| 13    | 0.18300 |  0.18008 |   21.7      \n",
      "| 14    | 0.18221 |  0.17902 |   23.4      \n",
      "| 15    | 0.18229 |  0.17881 |   25.0      \n",
      "| 16    | 0.18305 |  0.17957 |   26.7      \n",
      "| 17    | 0.18362 |  0.18017 |   28.3      \n",
      "| 18    | 0.18353 |  0.17908 |   30.0      \n",
      "| 19    | 0.18342 |  0.18155 |   31.6      \n",
      "| 20    | 0.18375 |  0.18004 |   33.3      \n",
      "| 21    | 0.18276 |  0.17791 |   34.9      \n",
      "| 22    | 0.18197 |  0.17856 |   36.6      \n",
      "| 23    | 0.18139 |  0.17735 |   38.3      \n",
      "| 24    | 0.18118 |  0.17555 |   40.0      \n",
      "| 25    | 0.18066 |  0.17547 |   41.7      \n",
      "| 26    | 0.18030 |  0.17626 |   43.4      \n",
      "| 27    | 0.18057 |  0.17726 |   45.0      \n",
      "| 28    | 0.18048 |  0.17554 |   46.7      \n",
      "| 29    | 0.18002 |  0.17510 |   48.4      \n",
      "| 30    | 0.18015 |  0.17636 |   50.1      \n",
      "| 31    | 0.18074 |  0.17608 |   51.8      \n",
      "| 32    | 0.18024 |  0.17487 |   53.5      \n",
      "| 33    | 0.17987 |  0.17639 |   55.2      \n",
      "| 34    | 0.18095 |  0.17482 |   56.8      \n",
      "| 35    | 0.18043 |  0.17517 |   58.5      \n",
      "| 36    | 0.18018 |  0.17717 |   60.2      \n",
      "| 37    | 0.17994 |  0.17565 |   61.9      \n",
      "| 38    | 0.17971 |  0.17547 |   63.6      \n",
      "| 39    | 0.17966 |  0.17536 |   65.3      \n",
      "| 40    | 0.18008 |  0.17530 |   67.0      \n",
      "| 41    | 0.18017 |  0.17613 |   68.7      \n",
      "| 42    | 0.17971 |  0.17463 |   70.4      \n",
      "| 43    | 0.17942 |  0.17635 |   72.0      \n",
      "| 44    | 0.17941 |  0.17660 |   73.7      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 45    | 0.17981 |  0.17624 |   75.4      \n",
      "| 46    | 0.17968 |  0.17491 |   77.1      \n",
      "| 47    | 0.17896 |  0.17561 |   78.8      \n",
      "| 48    | 0.17978 |  0.17571 |   80.6      \n",
      "| 49    | 0.18031 |  0.17612 |   82.2      \n",
      "| 50    | 0.18079 |  0.17666 |   83.9      \n",
      "| 51    | 0.17995 |  0.17594 |   85.7      \n",
      "| 52    | 0.17971 |  0.17666 |   87.3      \n",
      "| 53    | 0.17999 |  0.17848 |   89.0      \n",
      "| 54    | 0.18118 |  0.17916 |   90.7      \n",
      "| 55    | 0.18056 |  0.17732 |   92.4      \n",
      "| 56    | 0.18014 |  0.17614 |   94.1      \n",
      "| 57    | 0.18010 |  0.17484 |   95.9      \n",
      "| 58    | 0.18010 |  0.17520 |   97.6      \n",
      "| 59    | 0.17989 |  0.17788 |   99.3      \n",
      "| 60    | 0.18014 |  0.17678 |   100.9     \n",
      "| 61    | 0.18008 |  0.17615 |   102.7     \n",
      "| 62    | 0.18004 |  0.17897 |   104.3     \n",
      "Early stopping occured at epoch 62\n",
      "Training done in 104.328 seconds.\n",
      "---------------------------------------\n",
      "0.17462749379683518\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 20 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.22969 |  0.20275 |   1.7       \n",
      "| 2     | 0.19156 |  0.19742 |   3.4       \n",
      "| 3     | 0.18790 |  0.19410 |   5.0       \n",
      "| 4     | 0.18573 |  0.18925 |   6.7       \n",
      "| 5     | 0.18425 |  0.18761 |   8.4       \n",
      "| 6     | 0.18303 |  0.18734 |   10.1      \n",
      "| 7     | 0.18278 |  0.18723 |   11.8      \n",
      "| 8     | 0.18218 |  0.18651 |   13.5      \n",
      "| 9     | 0.18262 |  0.18571 |   15.2      \n",
      "| 10    | 0.18156 |  0.18523 |   16.9      \n",
      "| 11    | 0.18129 |  0.18557 |   18.6      \n",
      "| 12    | 0.18179 |  0.18541 |   20.3      \n",
      "| 13    | 0.18188 |  0.18621 |   22.0      \n",
      "| 14    | 0.18188 |  0.18638 |   23.6      \n",
      "| 15    | 0.18168 |  0.18704 |   25.3      \n",
      "| 16    | 0.18098 |  0.18511 |   27.0      \n",
      "| 17    | 0.18100 |  0.18495 |   28.7      \n",
      "| 18    | 0.18210 |  0.18489 |   30.4      \n",
      "| 19    | 0.18134 |  0.18438 |   32.2      \n",
      "| 20    | 0.18126 |  0.18381 |   33.9      \n",
      "| 21    | 0.18217 |  0.18488 |   35.6      \n",
      "| 22    | 0.18408 |  0.18613 |   37.3      \n",
      "| 23    | 0.18428 |  0.18586 |   39.0      \n",
      "| 24    | 0.18277 |  0.18569 |   40.7      \n",
      "| 25    | 0.18235 |  0.18508 |   42.4      \n",
      "| 26    | 0.18251 |  0.18485 |   44.1      \n",
      "| 27    | 0.18202 |  0.18518 |   45.8      \n",
      "| 28    | 0.18231 |  0.18569 |   47.5      \n",
      "| 29    | 0.18233 |  0.18485 |   49.2      \n",
      "| 30    | 0.18217 |  0.18409 |   50.9      \n",
      "| 31    | 0.18167 |  0.18398 |   52.6      \n",
      "| 32    | 0.18246 |  0.18452 |   54.3      \n",
      "| 33    | 0.18330 |  0.18774 |   56.0      \n",
      "| 34    | 0.18365 |  0.18371 |   57.7      \n",
      "| 35    | 0.18232 |  0.18546 |   59.3      \n",
      "| 36    | 0.18216 |  0.18392 |   61.0      \n",
      "| 37    | 0.18241 |  0.18463 |   62.7      \n",
      "| 38    | 0.18281 |  0.18607 |   64.4      \n",
      "| 39    | 0.18316 |  0.18553 |   66.1      \n",
      "| 40    | 0.18329 |  0.18489 |   67.8      \n",
      "| 41    | 0.18243 |  0.18750 |   69.4      \n",
      "| 42    | 0.18284 |  0.18929 |   71.1      \n",
      "| 43    | 0.18313 |  0.18569 |   72.8      \n",
      "| 44    | 0.18283 |  0.18579 |   74.5      \n",
      "| 45    | 0.18245 |  0.18414 |   76.2      \n",
      "| 46    | 0.18230 |  0.18554 |   77.8      \n",
      "| 47    | 0.18219 |  0.18622 |   79.5      \n",
      "| 48    | 0.18351 |  0.18818 |   81.2      \n",
      "| 49    | 0.18317 |  0.18568 |   82.9      \n",
      "| 50    | 0.18305 |  0.18621 |   84.6      \n",
      "| 51    | 0.18328 |  0.18533 |   86.3      \n",
      "| 52    | 0.18298 |  0.18499 |   88.0      \n",
      "| 53    | 0.18286 |  0.18450 |   89.6      \n",
      "| 54    | 0.18325 |  0.18817 |   91.3      \n",
      "Early stopping occured at epoch 54\n",
      "Training done in 91.281 seconds.\n",
      "---------------------------------------\n",
      "0.18371286068256013\n",
      "Device used : cuda\n",
      "Will train until validation stopping metric hasn't improved in 20 rounds.\n",
      "---------------------------------------\n",
      "| EPOCH |  train  |   valid  | total time (s)\n",
      "| 1     | 0.23046 |  0.20972 |   1.7       \n",
      "| 2     | 0.19075 |  0.20289 |   3.4       \n",
      "| 3     | 0.18716 |  0.19861 |   5.2       \n",
      "| 4     | 0.18664 |  0.19038 |   6.9       \n",
      "| 5     | 0.18462 |  0.19147 |   8.7       \n",
      "| 6     | 0.18389 |  0.18818 |   10.5      \n",
      "| 7     | 0.18388 |  0.18621 |   12.6      \n",
      "| 8     | 0.18337 |  0.18593 |   14.7      \n",
      "| 9     | 0.18251 |  0.18789 |   16.5      \n",
      "| 10    | 0.18314 |  0.18750 |   18.3      \n",
      "| 11    | 0.18306 |  0.18722 |   20.0      \n",
      "| 12    | 0.18222 |  0.18568 |   21.7      \n",
      "| 13    | 0.18226 |  0.18595 |   23.3      \n",
      "| 14    | 0.18168 |  0.18525 |   25.1      \n",
      "| 15    | 0.18143 |  0.18575 |   26.8      \n",
      "| 16    | 0.18223 |  0.18605 |   28.6      \n",
      "| 17    | 0.18169 |  0.18511 |   30.3      \n",
      "| 18    | 0.18081 |  0.18472 |   32.1      \n",
      "| 19    | 0.18078 |  0.18450 |   34.0      \n",
      "| 20    | 0.18072 |  0.18459 |   35.9      \n",
      "| 21    | 0.18038 |  0.18468 |   37.8      \n",
      "| 22    | 0.18040 |  0.18525 |   39.6      \n",
      "| 23    | 0.18089 |  0.18412 |   41.5      \n",
      "| 24    | 0.18089 |  0.18431 |   43.3      \n",
      "| 25    | 0.18070 |  0.18418 |   45.2      \n",
      "| 26    | 0.18043 |  0.18644 |   47.0      \n",
      "| 27    | 0.18046 |  0.18438 |   48.8      \n",
      "| 28    | 0.18058 |  0.18488 |   50.7      \n",
      "| 29    | 0.18028 |  0.18429 |   52.5      \n",
      "| 30    | 0.18050 |  0.18416 |   54.3      \n",
      "| 31    | 0.18013 |  0.18468 |   56.1      \n",
      "| 32    | 0.18043 |  0.18425 |   58.0      \n",
      "| 33    | 0.18085 |  0.18630 |   60.0      \n",
      "| 34    | 0.18088 |  0.18479 |   61.9      \n",
      "| 35    | 0.18106 |  0.18361 |   63.8      \n",
      "| 36    | 0.18072 |  0.18475 |   65.7      \n",
      "| 37    | 0.18036 |  0.18365 |   67.6      \n",
      "| 38    | 0.18021 |  0.18373 |   69.4      \n",
      "| 39    | 0.18009 |  0.18351 |   71.4      \n",
      "| 40    | 0.18071 |  0.18382 |   73.3      \n",
      "| 41    | 0.18024 |  0.18314 |   75.4      \n",
      "| 42    | 0.17981 |  0.18405 |   77.3      \n",
      "| 43    | 0.17977 |  0.18433 |   79.1      \n",
      "| 44    | 0.18058 |  0.18431 |   80.9      \n",
      "| 45    | 0.18028 |  0.18396 |   82.9      \n",
      "| 46    | 0.18000 |  0.18468 |   84.7      \n",
      "| 47    | 0.17990 |  0.18393 |   86.5      \n",
      "| 48    | 0.17973 |  0.18349 |   88.5      \n",
      "| 49    | 0.17992 |  0.18420 |   90.4      \n",
      "| 50    | 0.18023 |  0.18424 |   92.2      \n",
      "| 51    | 0.18068 |  0.18531 |   94.0      \n",
      "| 52    | 0.18062 |  0.18629 |   95.8      \n",
      "| 53    | 0.18049 |  0.18387 |   97.7      \n",
      "| 54    | 0.18017 |  0.18373 |   99.7      \n",
      "| 55    | 0.18023 |  0.18516 |   101.7     \n",
      "| 56    | 0.18045 |  0.18405 |   103.6     \n",
      "| 57    | 0.18177 |  0.18564 |   105.3     \n",
      "| 58    | 0.18066 |  0.18438 |   107.2     \n",
      "| 59    | 0.18022 |  0.18467 |   109.0     \n",
      "| 60    | 0.17999 |  0.18426 |   110.7     \n",
      "| 61    | 0.18004 |  0.18445 |   112.5     \n",
      "Early stopping occured at epoch 61\n",
      "Training done in 112.527 seconds.\n",
      "---------------------------------------\n",
      "0.1831380933456123\n"
     ]
    }
   ],
   "source": [
    "evals_results = []\n",
    "clfs = []\n",
    "for _train, _ytr, _test, _yte in split_and_preprocess(X_train, y_train):\n",
    "    clf = TabNetClassifier(n_d=8, n_a=8, cat_idxs=cat_idxs, cat_dims=cat_dims,cat_emb_dim=1)\n",
    "    evals_result = clf.fit(\n",
    "        X_train=_train, y_train=_ytr,\n",
    "        X_valid=_test, y_valid=_yte,\n",
    "        max_epochs=1000, patience=20,\n",
    "        batch_size=2048, virtual_batch_size=512,\n",
    "        num_workers=4,\n",
    "        drop_last=False\n",
    "    )\n",
    "    print(evals_result)\n",
    "    evals_results.append(evals_result)\n",
    "    clfs.append(deepcopy(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8613843099276048\n",
      "0.8613252408333787\n",
      "0.8602035749092464\n",
      "0.861547081881204\n",
      "0.8618849162454717\n",
      "0.8645837099626446\n",
      "0.8557742439819829\n",
      "0.8600269123559676\n"
     ]
    }
   ],
   "source": [
    "for c in clfs:\n",
    "    preds = c.predict_proba(X_test)\n",
    "    print(roc_auc_score(y_score=preds[:,1], y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL TEST SCORE FOR census-income : 0.8647627216614574\n"
     ]
    }
   ],
   "source": [
    "preds = 0\n",
    "for c in clfs:\n",
    "    preds += c.predict_proba(X_test) / len(clfs)\n",
    "    \n",
    "test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n",
    "\n",
    "print(f\"FINAL TEST SCORE FOR {dataset_name} : {test_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global explainability : feat importance summing to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.67917179e-01, 5.32388244e-02, 1.09645185e-01, 8.12863796e-02,\n",
       "       2.55622089e-02, 3.83718294e-06, 3.23123805e-01, 5.80269946e-03,\n",
       "       7.44764352e-09, 3.34198748e-02])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local explainability and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix, masks = clf.explain(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].imshow(masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
